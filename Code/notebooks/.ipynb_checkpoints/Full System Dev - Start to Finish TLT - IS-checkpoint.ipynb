{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IS\n",
    "Goal: Walk through all steps to pivot date and then continue day by day trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pylab as plt\n",
    "from pandas.tseries.offsets import BDay\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, TimeSeriesSplit\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom libraries\n",
    "from Code.lib.plot_utils import PlotUtility\n",
    "from Code.lib.time_utils import TimeUtility\n",
    "from Code.lib.retrieve_data import DataRetrieve, ComputeTarget\n",
    "from Code.lib.retrieve_system_info import TradingSystemUtility\n",
    "from Code.lib.candle_indicators import CandleIndicators\n",
    "from Code.lib.transformers import Transformers\n",
    "from Code.lib.ta_momentum_studies import TALibMomentumStudies\n",
    "from Code.lib.model_utils import ModelUtility, TimeSeriesSplitImproved\n",
    "from Code.lib.feature_generator import FeatureGenerator\n",
    "from Code.utilities.stat_tests import stationarity_tests\n",
    "from Code.lib.config import current_feature, feature_dict\n",
    "from Code.models import models_utils\n",
    "from Code.lib.model_algos import AlgoUtility\n",
    "\n",
    "plotIt = PlotUtility()\n",
    "timeUtil = TimeUtility()\n",
    "ct = ComputeTarget()\n",
    "candle_ind = CandleIndicators()\n",
    "dSet = DataRetrieve()\n",
    "sysUtil = TradingSystemUtility()\n",
    "taLibMomSt = TALibMomentumStudies()\n",
    "transf = Transformers()\n",
    "modelUtil = ModelUtility()\n",
    "featureGen = FeatureGenerator()\n",
    "dSet = DataRetrieve()\n",
    "modelAlgo = AlgoUtility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish and save system metadata\n",
    "If new issue  \n",
    "    Create system name  \n",
    "    Create system dict  \n",
    "Define IS-OOS, pivot date parameters  \n",
    "\n",
    "Alternative  \n",
    "Identify system name if existing\n",
    "Otherwise, if system name is blank, it will create new system\n",
    "Read system dict..it will contain entries if previous system exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = \"TLT-Long-system-6571-V1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing system\n",
      "C:\\Users\\kruegkj\\Documents\\GitHub\\QuantTradingSys\\Code\\notebooks\\TLT-Long-system-6571-V1\n",
      "{'direction': 'Long', 'is_oos_ratio': 4, 'issue': 'TLT', 'oos_months': 4, 'pivotDate': '2019-01-03', 'segments': 1, 'system_name': 'TLT-Long-system-6571-V1', 'ver_num': 1}\n"
     ]
    }
   ],
   "source": [
    "if system_name == \"\":\n",
    "    # set some defaults for now \n",
    "    print(\"New system\")\n",
    "    issue = \"TLT\"\n",
    "    direction = \"Long\"\n",
    "    ver_num = 1\n",
    "    system_dict = sysUtil.get_system_dict(system_name, issue, direction, ver_num)\n",
    "\n",
    "    pivotDate = str(datetime.date(2019, 1, 3))\n",
    "    is_oos_ratio = 4\n",
    "    oos_months = 4\n",
    "    segments = 1\n",
    "\n",
    "    system_dict['pivotDate'] = pivotDate\n",
    "    system_dict['is_oos_ratio'] = is_oos_ratio\n",
    "    system_dict['oos_months'] = oos_months\n",
    "    system_dict['segments'] = segments\n",
    "\n",
    "    system_name = system_dict['system_name']\n",
    "    system_directory = sysUtil.get_system_dir(system_name)\n",
    "\n",
    "    dSet.save_json('system_dict.json', system_directory, system_dict)\n",
    "else:\n",
    "    print(\"Existing system\")\n",
    "    system_directory = sysUtil.get_system_dir(system_name)\n",
    "    print(system_directory)\n",
    "    file_name = 'system_dict.json'\n",
    "    system_dict = dSet.load_json(system_directory, file_name)\n",
    "\n",
    "print(system_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### At some point, need to address time parameters\n",
    "Load from system dict or declare\n",
    "Am I re-calculating params from function call or saving/retrieving from system dict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Set IS-OOS parameters\n",
    "from datetime import datetime\n",
    "pivotDate = system_dict['pivotDate']\n",
    "pivotDate = datetime.strptime(pivotDate, '%Y-%m-%d')\n",
    "print(pivotDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved data series for TLT\n"
     ]
    }
   ],
   "source": [
    "issue = system_dict['issue']\n",
    "df = dSet.read_issue_data(issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set date range and target\n",
    "dataLoadStartDate = df.Date[0]\n",
    "lastRow = df.shape[0]\n",
    "dataLoadEndDate = df.Date[lastRow-1]\n",
    "dataSet = dSet.set_date_range(df, dataLoadStartDate,dataLoadEndDate)\n",
    "# Resolve any NA's for now\n",
    "dataSet.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#set beLong level\n",
    "beLongThreshold = 0.000\n",
    "dataSet = ct.setTarget(dataSet, \"Long\", beLongThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "      <th>gainAhead</th>\n",
       "      <th>beLong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>121.66</td>\n",
       "      <td>121.7500</td>\n",
       "      <td>121.20</td>\n",
       "      <td>121.5550</td>\n",
       "      <td>121.5550</td>\n",
       "      <td>7939591.0</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>121.15</td>\n",
       "      <td>121.3900</td>\n",
       "      <td>120.91</td>\n",
       "      <td>121.1000</td>\n",
       "      <td>121.1000</td>\n",
       "      <td>4757681.0</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>122.14</td>\n",
       "      <td>122.1957</td>\n",
       "      <td>121.65</td>\n",
       "      <td>121.7633</td>\n",
       "      <td>121.7633</td>\n",
       "      <td>2496779.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open      High     Low     Close  AdjClose     Volume  \\\n",
       "2019-02-12  121.66  121.7500  121.20  121.5550  121.5550  7939591.0   \n",
       "2019-02-13  121.15  121.3900  120.91  121.1000  121.1000  4757681.0   \n",
       "2019-02-14  122.14  122.1957  121.65  121.7633  121.7633  2496779.0   \n",
       "\n",
       "            gainAhead  beLong  \n",
       "2019-02-12  -0.003743      -1  \n",
       "2019-02-13   0.005477       1  \n",
       "2019-02-14   0.000000      -1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick review of loaded data\n",
    "dataSet.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features\n",
    "Features will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dict = {} # initialize\n",
    "input_dict = {'f1': \n",
    "              {'fname' : 'PPO', \n",
    "               'params' : [2,5],\n",
    "               'transform' : ['Normalized', 20]\n",
    "               },\n",
    "              'f2': \n",
    "              {'fname' : 'RSI', \n",
    "               'params' : [2],\n",
    "               'transform' : ['Normalized', 20]\n",
    "               },\n",
    "              'f3': \n",
    "              {'fname' : 'CMO', \n",
    "               'params' : [5],\n",
    "               'transform' : ['Normalized', 20]\n",
    "               },\n",
    "              'f4': \n",
    "              {'fname' : 'CCI', \n",
    "               'params' : [10],\n",
    "               'transform' : ['Normalized', 20]\n",
    "               },\n",
    "              'f5': \n",
    "              {'fname' : 'UltimateOscillator', \n",
    "               'params' : [10, 20, 30],\n",
    "               'transform' : ['Normalized', 20]\n",
    "               },\n",
    "              'f6': \n",
    "              {'fname' : 'ROC', \n",
    "               'params' : [10],\n",
    "               'transform' : ['Normalized', 20]\n",
    "               },\n",
    "              'f7': \n",
    "                  {'fname' : 'Lag', \n",
    "                   'params' : ['Close', 3],\n",
    "                   'transform' : ['Normalized', 20]\n",
    "                   },\n",
    "              'f8': \n",
    "                  {'fname' : 'Lag', \n",
    "                   'params' : ['Close', 5],\n",
    "                   'transform' : ['Normalized', 20]\n",
    "                   },\n",
    "              'f9': \n",
    "                  {'fname' : 'ChaikinADOSC', \n",
    "                   'params' : [4, 10],\n",
    "                   'transform' : ['Normalized', 20]\n",
    "                   },\n",
    "              'f10': \n",
    "                  {'fname' : 'kaufman_AMA', \n",
    "                   'params' : [4],\n",
    "                   'transform' : ['Normalized', 20]\n",
    "                   }\n",
    "             }    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet2 = featureGen.generate_features(dataSet, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataSet2 = transf.normalizer(dataSet, 'Volume', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_json('input_dict.json', input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data locally\n",
    "Save data to a pickle file in the data dir  \n",
    "Name format: raw-features-<<'system_name'>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Dataset of analysis\n",
    "print(\"====Saving dataSet====\\n\")\n",
    "print(system_directory)\n",
    "print(system_name)\n",
    "file_title = \"raw-features-\" + system_name + \".pkl\"\n",
    "file_name = os.path.join(system_directory, file_title)\n",
    "dataSet2.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine correlation of features\n",
    "Improve this to identify and drop features with corr value higher than 0.3 Look at other methods to identify features and their contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns to drop from feature_dict\n",
    "col_vals = [k for k,v in feature_dict.items() if v == 'Drop']\n",
    "# And set OHLC, etc., to Drop for cleaner correlation analysis\n",
    "to_drop = ['Open','High','Low', 'gainAhead', 'Close', 'beLong', 'Volume', 'AdjClose']\n",
    "for x in to_drop:\n",
    "    col_vals.append(x)\n",
    "mmData = dSet.drop_columns(dataSet2, col_vals)\n",
    "\n",
    "plotIt.correlation_matrix(mmData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine and drop features with corr value > 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = mmData.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.85\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "print('Column(s) to drop: %s' % to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are columns to Drop, change feature dict to indicate Drop\n",
    "if len(to_drop) > 0:\n",
    "    for x in to_drop:\n",
    "        feature_dict[x] = 'Drop'\n",
    "    print(feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save feature_dict to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_json('feature_dict.json', feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROGRESS SO FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Placeholder) Examine feature importance of remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processing dates for IS and OOS; set start date for model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date splits\n",
    "isOosDates = timeUtil.is_oos_data_split(issue, pivotDate, is_oos_ratio, oos_months, segments)\n",
    "dataLoadStartDate = isOosDates[0]\n",
    "is_start_date = isOosDates[1]\n",
    "oos_start_date = isOosDates[2]\n",
    "is_months = isOosDates[3]\n",
    "is_end_date = isOosDates[4]\n",
    "oos_end_date = isOosDates[5]\n",
    "\n",
    "modelStartDate = is_start_date\n",
    "modelEndDate = modelStartDate + relativedelta(months=is_months)\n",
    "print(\"Issue: \" + issue)\n",
    "print(\"IS Start date: \" + str(modelStartDate) + \"  IS End date: \" + str(modelEndDate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep data sets for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "mmData = dataSet2.loc[modelStartDate:modelEndDate].copy()\n",
    "# EV related\n",
    "evData = dataSet2.loc[modelStartDate:modelEndDate].copy()\n",
    "\n",
    "col_vals = [k for k,v in feature_dict.items() if v == 'Drop']\n",
    "to_drop = ['Open','High','Low', 'gainAhead', 'Close', 'Volume', 'AdjClose']\n",
    "for x in to_drop:\n",
    "    col_vals.append(x)\n",
    "mmData = dSet.drop_columns(mmData, col_vals)\n",
    "nrows = mmData.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# ML section\n",
    "######################\n",
    "#  Make 'iterations' index vectors for the train-test split\n",
    "iterations = 100\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "dX, dy = modelUtil.prepare_for_classification(mmData)        \n",
    "\n",
    "tscvi = TimeSeriesSplitImproved(n_splits=8)\n",
    "\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mmData.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_model = {\"RF\": modelAlgo.setRFClass(min_samples_split=20,\n",
    "                                       n_estimators=200,\n",
    "                                       max_features=None\n",
    "                                       ),\n",
    "            \"KNN\": modelAlgo.setKNNClass(n_neighbors=5),\n",
    "            \"SVM\": modelAlgo.setSVMClass(),\n",
    "            \"AdaBoost\": modelAlgo.setAdaBoostClass(learning_rate=0.2,\n",
    "                                                   n_estimators=500\n",
    "                                                  ),\n",
    "            \"GTB\": modelAlgo.setGTBClass(learning_rate=0.05,\n",
    "                                         subsample=0.5,\n",
    "                                         max_depth=6,\n",
    "                                         n_estimators=10\n",
    "                                        ),\n",
    "            \"QDA\": modelAlgo.setQDAClass()}\n",
    "for key, value in to_model.items():\n",
    "    modelname = key\n",
    "    model = value\n",
    "    info_dict = {'issue':issue,\n",
    "                 'modelStartDate':modelStartDate,\n",
    "                 'modelEndDate':modelEndDate,\n",
    "                 'modelname':modelname,\n",
    "                 'nrows':nrows,\n",
    "                 'system_name':system_name\n",
    "                }\n",
    "    print(modelname)\n",
    "    print(model)\n",
    "\n",
    "    model_results, fit_model = modelUtil.model_and_test(dX,\n",
    "                                                        dy,\n",
    "                                                        model,\n",
    "                                                        model_results,\n",
    "                                                        tscvi,\n",
    "                                                        info_dict,\n",
    "                                                        evData\n",
    "                                                       )\n",
    "    \n",
    "    # save Dataset of analysis\n",
    "    print(\"====Saving model====\\n\")\n",
    "    file_title = \"fit-model-\" + modelname + \"-IS-\" + system_name + \".sav\"\n",
    "    file_name = os.path.join(system_directory, file_title)\n",
    "    pickle.dump(fit_model, open(file_name, 'wb'))\n",
    "    print(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop ended, print results\n",
    "df = pd.DataFrame(model_results)\n",
    "df = df[['Issue',\n",
    "         'StartDate',\n",
    "         'EndDate',\n",
    "         'Model',\n",
    "         'Rows',\n",
    "         'beLongCount',\n",
    "         'Features',\n",
    "         'IS-Accuracy',\n",
    "         'IS-Precision',\n",
    "         'IS-RMC',\n",
    "         'IS-RF',\n",
    "         'IS-NPV',\n",
    "         'IS-MCC',\n",
    "         'IS-EV',\n",
    "         'OOS-Accuracy',\n",
    "         'OOS-Precision',\n",
    "         'OOS-RMC',\n",
    "         'OOS-RF',\n",
    "         'OOS-NPV',\n",
    "         'OOS-MCC',\n",
    "         'OOS-EV',\n",
    "        ]]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results\n",
    "import datetime\n",
    "dirext = system_name + '_start_' + str(dataLoadStartDate.strftime(\"%Y-%m-%d\")) + '_end_' + str(pivotDate.strftime(\"%Y-%m-%d\")) + '_' + datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(dirext)\n",
    "filename = dirext + \"IS_model_results.csv\"\n",
    "df.to_csv(system_directory+ \"\\\\\" + filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "system_dict['best_model'] = \"SVM\"\n",
    "save_json('system_dict.json', system_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current State\n",
    "1. All models saved\n",
    "2. Need to manually select best performing model\n",
    "3. What is the metric for best performing model?\n",
    "4. Do I need to add MAE, MAPE, etc. or other? How?\n",
    "\n",
    "Files from analysis:\n",
    "1. feature_dict\n",
    "2. system_dict\n",
    "3. <model results>\n",
    "4. saved models\n",
    "5. raw data with features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to the next sheet...to simulate starting the next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
